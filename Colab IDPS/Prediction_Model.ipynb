{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW0jWBvn9nvg",
        "outputId": "f721ec30-e321-4523-ae06-9b4b6d5f8d8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üîç Linear Regression Performance:\n",
            "R¬≤ Score: 0.9973\n",
            "MAE: 0.0420\n",
            "RMSE: 0.2050\n",
            "\n",
            "üîç Random Forest Performance:\n",
            "R¬≤ Score: 0.9984\n",
            "MAE: 0.0340\n",
            "RMSE: 0.1845\n",
            "\n",
            "üîç XGBoost Performance:\n",
            "R¬≤ Score: 0.9989\n",
            "MAE: 0.0302\n",
            "RMSE: 0.1737\n",
            "\n",
            "üîç MLPRegressor Performance:\n",
            "R¬≤ Score: 0.9963\n",
            "MAE: 0.0531\n",
            "RMSE: 0.2305\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "üîç Keras DNN Performance:\n",
            "R¬≤ Score: 0.9977\n",
            "MAE: 0.0390\n",
            "RMSE: 0.1976\n",
            "\n",
            "üìä Summary of All Models:\n",
            "               Model        R2       MAE      RMSE\n",
            "0            XGBoost  0.998853  0.030182  0.173730\n",
            "1      Random Forest  0.998358  0.034043  0.184507\n",
            "2          Keras DNN  0.997731  0.039037  0.197579\n",
            "3  Linear Regression  0.997310  0.042005  0.204951\n",
            "4       MLPRegressor  0.996325  0.053107  0.230450\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# === Load and prepare dataset ===\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/CWDS_DEMO/BATADAL Datasets/BATADAL_normal.csv\")  # Normal behavior data\n",
        "sensor_cols = [col for col in df.columns if col not in ['DATETIME', 'ATT_FLAG']]\n",
        "\n",
        "# Predict future value of target variable (e.g., L_T1)\n",
        "df['L_T1_next'] = df['L_T1'].shift(-1)\n",
        "df.dropna(inplace=True)  # Remove last row with NaN target\n",
        "\n",
        "X = df[sensor_cols]\n",
        "y = df['L_T1_next']\n",
        "\n",
        "# Scale features\n",
        "scaler = MinMaxScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# === Helper function to evaluate models ===\n",
        "def evaluate(model_name, y_true, y_pred):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_absolute_error(y_true, y_pred))\n",
        "    print(f\"\\nüîç {model_name} Performance:\")\n",
        "    print(f\"R¬≤ Score: {r2:.4f}\")\n",
        "    print(f\"MAE: {mae:.4f}\")\n",
        "    print(f\"RMSE: {rmse:.4f}\")\n",
        "    return model_name, r2, mae, rmse\n",
        "\n",
        "results = []\n",
        "\n",
        "# === 1. Linear Regression ===\n",
        "lr = LinearRegression()\n",
        "lr.fit(X_train, y_train)\n",
        "results.append(evaluate(\"Linear Regression\", y_test, lr.predict(X_test)))\n",
        "\n",
        "# === 2. Random Forest ===\n",
        "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf.fit(X_train, y_train)\n",
        "results.append(evaluate(\"Random Forest\", y_test, rf.predict(X_test)))\n",
        "\n",
        "# === 3. XGBoost ===\n",
        "xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
        "xgb.fit(X_train, y_train)\n",
        "results.append(evaluate(\"XGBoost\", y_test, xgb.predict(X_test)))\n",
        "\n",
        "# === 4. MLP Regressor (SKLearn) ===\n",
        "mlp = MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42)\n",
        "mlp.fit(X_train, y_train)\n",
        "results.append(evaluate(\"MLPRegressor\", y_test, mlp.predict(X_test)))\n",
        "\n",
        "# === 5. Keras Feedforward DNN ===\n",
        "dnn = Sequential([\n",
        "    Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "dnn.compile(optimizer='adam', loss='mae')\n",
        "dnn.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "y_pred_dnn = dnn.predict(X_test).flatten()\n",
        "results.append(evaluate(\"Keras DNN\", y_test, y_pred_dnn))\n",
        "\n",
        "# === Show all results ===\n",
        "results_df = pd.DataFrame(results, columns=[\"Model\", \"R2\", \"MAE\", \"RMSE\"])\n",
        "print(\"\\nüìä Summary of All Models:\")\n",
        "print(results_df.sort_values(\"R2\", ascending=False).reset_index(drop=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Several Targets"
      ],
      "metadata": {
        "id": "BZBi5Q4kCytl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# === Load and prepare dataset ===\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/CWDS_DEMO/BATADAL Datasets/BATADAL_normal.csv\")  # Normal behavior data\n",
        "sensor_cols = [col for col in df.columns if col not in ['DATETIME', 'ATT_FLAG']]\n",
        "\n",
        "# === List of target sensors to test ===\n",
        "target_sensors = ['L_T1', 'L_T2', 'P_J280']\n",
        "\n",
        "# === Helper function to evaluate models ===\n",
        "def evaluate(model_name, y_true, y_pred):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_absolute_error(y_true, y_pred))\n",
        "    return model_name, r2, mae, rmse\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for target_sensor in target_sensors:\n",
        "    print(f\"\\n=== Evaluating target: {target_sensor}_next ===\")\n",
        "    df_temp = df.copy()\n",
        "    df_temp[f'{target_sensor}_next'] = df_temp[target_sensor].shift(-1)\n",
        "    df_temp.dropna(inplace=True)\n",
        "\n",
        "    X = df_temp[sensor_cols]\n",
        "    y = df_temp[f'{target_sensor}_next']\n",
        "\n",
        "    # Scale features\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Store metrics per model\n",
        "    results = []\n",
        "\n",
        "    # 1. Linear Regression\n",
        "    lr = LinearRegression()\n",
        "    lr.fit(X_train, y_train)\n",
        "    results.append(evaluate(\"Linear Regression\", y_test, lr.predict(X_test)))\n",
        "\n",
        "    # 2. Random Forest\n",
        "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    results.append(evaluate(\"Random Forest\", y_test, rf.predict(X_test)))\n",
        "\n",
        "    # 3. XGBoost\n",
        "    xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
        "    xgb.fit(X_train, y_train)\n",
        "    results.append(evaluate(\"XGBoost\", y_test, xgb.predict(X_test)))\n",
        "\n",
        "    # 4. MLP Regressor\n",
        "    mlp = MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42)\n",
        "    mlp.fit(X_train, y_train)\n",
        "    results.append(evaluate(\"MLPRegressor\", y_test, mlp.predict(X_test)))\n",
        "\n",
        "    # 5. Keras DNN\n",
        "    dnn = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    dnn.compile(optimizer='adam', loss='mae')\n",
        "    dnn.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "    y_pred_dnn = dnn.predict(X_test).flatten()\n",
        "    results.append(evaluate(\"Keras DNN\", y_test, y_pred_dnn))\n",
        "\n",
        "    # Add target label and store\n",
        "    for res in results:\n",
        "        all_results.append((res[0], target_sensor, res[1], res[2], res[3]))\n",
        "\n",
        "# === Convert to DataFrame and aggregate ===\n",
        "all_df = pd.DataFrame(all_results, columns=[\"Model\", \"Target\", \"R2\", \"MAE\", \"RMSE\"])\n",
        "\n",
        "print(\"\\nüìä Full Results by Target:\")\n",
        "print(all_df)\n",
        "\n",
        "summary = all_df.groupby(\"Model\").mean().reset_index()\n",
        "print(\"\\nüìà Average Performance Across Targets:\")\n",
        "print(summary.sort_values(\"R2\", ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z9ld9CggC1uo",
        "outputId": "f33b5df2-40b7-4922-c07a-5a2b83034e1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluating target: L_T1_next ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\n",
            "=== Evaluating target: L_T2_next ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "=== Evaluating target: P_J280_next ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m55/55\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "üìä Full Results by Target:\n",
            "                Model  Target         R2       MAE      RMSE\n",
            "0   Linear Regression    L_T1   0.997310  0.042005  0.204951\n",
            "1       Random Forest    L_T1   0.998358  0.034043  0.184507\n",
            "2             XGBoost    L_T1   0.998853  0.030182  0.173730\n",
            "3        MLPRegressor    L_T1   0.996325  0.053107  0.230450\n",
            "4           Keras DNN    L_T1   0.998117  0.037194  0.192858\n",
            "5   Linear Regression    L_T2   0.991543  0.090447  0.300745\n",
            "6       Random Forest    L_T2   0.997504  0.048041  0.219183\n",
            "7             XGBoost    L_T2   0.998225  0.044987  0.212101\n",
            "8        MLPRegressor    L_T2   0.993631  0.085258  0.291990\n",
            "9           Keras DNN    L_T2   0.995885  0.057627  0.240055\n",
            "10  Linear Regression  P_J280   0.864456  0.001320  0.036334\n",
            "11      Random Forest  P_J280   0.972027  0.000378  0.019454\n",
            "12            XGBoost  P_J280   0.966822  0.000419  0.020481\n",
            "13       MLPRegressor  P_J280 -16.392766  0.019797  0.140702\n",
            "14          Keras DNN  P_J280  -2.001980  0.009590  0.097929\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "agg function failed [how->mean,dtype->object]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_agg_py_fallback\u001b[0;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1941\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1942\u001b[0;31m             \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_grouper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magg_series\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreserve_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1943\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36magg_series\u001b[0;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[1;32m    863\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate_series_pure_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/ops.py\u001b[0m in \u001b[0;36m_aggregate_series_pure_python\u001b[0;34m(self, obj, func)\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplitter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2453\u001b[0m                 \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2454\u001b[0;31m                 \u001b[0malt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2455\u001b[0m                 \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6548\u001b[0m     ):\n\u001b[0;32m-> 6549\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12419\u001b[0m     ) -> Series | float:\n\u001b[0;32m> 12420\u001b[0;31m         return self._stat_function(\n\u001b[0m\u001b[1;32m  12421\u001b[0m             \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnanops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnanmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m> 12377\u001b[0;31m         return self._reduce(\n\u001b[0m\u001b[1;32m  12378\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   6456\u001b[0m                 )\n\u001b[0;32m-> 6457\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mf\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnew_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 404\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskipna\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskipna\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    719\u001b[0m     \u001b[0mthe_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m     \u001b[0mthe_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ensure_numeric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mthe_sum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/nanops.py\u001b[0m in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1700\u001b[0m             \u001b[0;31m# GH#44008, GH#36703 avoid casting e.g. strings to numeric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1701\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Could not convert string '{x}' to numeric\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1702\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Could not convert string 'L_T1L_T2P_J280' to numeric",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-458128dbf33b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m \u001b[0msummary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüìà Average Performance Across Targets:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"R2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mmean\u001b[0;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[1;32m   2450\u001b[0m             )\n\u001b[1;32m   2451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2452\u001b[0;31m             result = self._cython_agg_general(\n\u001b[0m\u001b[1;32m   2453\u001b[0m                 \u001b[0;34m\"mean\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2454\u001b[0m                 \u001b[0malt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnumeric_only\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m   1996\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mnew_mgr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrouped_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wrap_agged_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_mgr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"idxmin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"idxmax\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mgrouped_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1467\u001b[0m                 \u001b[0;31m#  while others do not.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1468\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0msb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mblk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1469\u001b[0;31m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1470\u001b[0m                     \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, **kwargs)\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \"\"\"\n\u001b[0;32m--> 393\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_coerce_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36marray_func\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1994\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0malt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1995\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agg_py_fallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1996\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36m_agg_py_fallback\u001b[0;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1944\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"agg function failed [how->{how},dtype->{ser.dtype}]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1945\u001b[0m             \u001b[0;31m# preserve the kind of exception that raised\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1946\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1948\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: agg function failed [how->mean,dtype->object]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Convert to DataFrame and aggregate ===\n",
        "all_df = pd.DataFrame(all_results, columns=[\"Model\", \"Target\", \"R2\", \"MAE\", \"RMSE\"])\n",
        "\n",
        "print(\"\\nüìä Full Results by Target:\")\n",
        "print(all_df)\n",
        "\n",
        "summary = all_df.groupby(\"Model\")[[\"R2\", \"MAE\", \"RMSE\"]].mean().reset_index()\n",
        "print(\"\\nüìà Average Performance Across Targets:\")\n",
        "print(summary.sort_values(\"R2\", ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ijkyEpKAEY1T",
        "outputId": "cbd989be-0660-45c9-ee42-2dccff0d5c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Full Results by Target:\n",
            "                Model  Target         R2       MAE      RMSE\n",
            "0   Linear Regression    L_T1   0.997310  0.042005  0.204951\n",
            "1       Random Forest    L_T1   0.998358  0.034043  0.184507\n",
            "2             XGBoost    L_T1   0.998853  0.030182  0.173730\n",
            "3        MLPRegressor    L_T1   0.996325  0.053107  0.230450\n",
            "4           Keras DNN    L_T1   0.998117  0.037194  0.192858\n",
            "5   Linear Regression    L_T2   0.991543  0.090447  0.300745\n",
            "6       Random Forest    L_T2   0.997504  0.048041  0.219183\n",
            "7             XGBoost    L_T2   0.998225  0.044987  0.212101\n",
            "8        MLPRegressor    L_T2   0.993631  0.085258  0.291990\n",
            "9           Keras DNN    L_T2   0.995885  0.057627  0.240055\n",
            "10  Linear Regression  P_J280   0.864456  0.001320  0.036334\n",
            "11      Random Forest  P_J280   0.972027  0.000378  0.019454\n",
            "12            XGBoost  P_J280   0.966822  0.000419  0.020481\n",
            "13       MLPRegressor  P_J280 -16.392766  0.019797  0.140702\n",
            "14          Keras DNN  P_J280  -2.001980  0.009590  0.097929\n",
            "\n",
            "üìà Average Performance Across Targets:\n",
            "               Model        R2       MAE      RMSE\n",
            "3      Random Forest  0.989296  0.027487  0.141048\n",
            "4            XGBoost  0.987967  0.025196  0.135437\n",
            "1  Linear Regression  0.951103  0.044591  0.180677\n",
            "0          Keras DNN -0.002659  0.034804  0.176947\n",
            "2       MLPRegressor -4.800936  0.052721  0.221047\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "# Save the trained XGBoost model\n",
        "joblib.dump(xgb, \"xgboost_predictor.pkl\")\n",
        "\n",
        "# Save the list of features used\n",
        "joblib.dump(sensor_cols, \"xgboost_features.pkl\")\n",
        "\n",
        "print(\"‚úÖ XGBoost model and feature list saved.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Epu25-blMXq7",
        "outputId": "6109b421-8d89-48c4-c82a-2e9b6a2a3374"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ XGBoost model and feature list saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WITH AREAL'S DATASET"
      ],
      "metadata": {
        "id": "MLLuKB__7iKw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# === Load and prepare dataset ===\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/CWDS_DEMO/Areal Dataset/Data without attacks.xlsx\")  # Normal behavior data\n",
        "sensor_cols = [col for col in df.columns if col not in ['Date ','Hours', 'ATT_FLAG']]\n",
        "\n",
        "# === List of target sensors to test ===\n",
        "target_sensors = ['TANKLEVEL','OUTPUTFLOW','RESERVETANKVOLUME']\n",
        "\n",
        "# === Helper function to evaluate models ===\n",
        "def evaluate(model_name, y_true, y_pred):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_absolute_error(y_true, y_pred))\n",
        "    return model_name, r2, mae, rmse\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for target_sensor in target_sensors:\n",
        "    print(f\"\\n=== Evaluating target: {target_sensor}_next ===\")\n",
        "    df_temp = df.copy()\n",
        "    df_temp[f'{target_sensor}_next'] = df_temp[target_sensor].shift(-1)\n",
        "    df_temp.dropna(inplace=True)\n",
        "\n",
        "    X = df_temp[sensor_cols]\n",
        "    y = df_temp[f'{target_sensor}_next']\n",
        "\n",
        "    # Scale features\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Store metrics per model\n",
        "    results = []\n",
        "\n",
        "    # 1. Linear Regression\n",
        "    lr = LinearRegression()\n",
        "    lr.fit(X_train, y_train)\n",
        "    results.append(evaluate(\"Linear Regression\", y_test, lr.predict(X_test)))\n",
        "\n",
        "    # 2. Random Forest\n",
        "    rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "    rf.fit(X_train, y_train)\n",
        "    results.append(evaluate(\"Random Forest\", y_test, rf.predict(X_test)))\n",
        "\n",
        "    # 3. XGBoost\n",
        "    xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
        "    xgb.fit(X_train, y_train)\n",
        "    results.append(evaluate(\"XGBoost\", y_test, xgb.predict(X_test)))\n",
        "\n",
        "    # 4. MLP Regressor\n",
        "    mlp = MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42)\n",
        "    mlp.fit(X_train, y_train)\n",
        "    results.append(evaluate(\"MLPRegressor\", y_test, mlp.predict(X_test)))\n",
        "\n",
        "    # 5. Keras DNN\n",
        "    dnn = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    dnn.compile(optimizer='adam', loss='mae')\n",
        "    dnn.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "    y_pred_dnn = dnn.predict(X_test).flatten()\n",
        "    results.append(evaluate(\"Keras DNN\", y_test, y_pred_dnn))\n",
        "\n",
        "    # Add target label and store\n",
        "    for res in results:\n",
        "        all_results.append((res[0], target_sensor, res[1], res[2], res[3]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hf9P79H7mOh",
        "outputId": "1b7dbd5d-0f61-4345-eaa7-a7bde4773c95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Evaluating target: TANKLEVEL_next ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m72/72\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "=== Evaluating target: OUTPUTFLOW_next ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m72/72\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
            "\n",
            "=== Evaluating target: RESERVETANKVOLUME_next ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m72/72\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Convert to DataFrame and aggregate ===\n",
        "all_df = pd.DataFrame(all_results, columns=[\"Model\", \"Target\", \"R2\", \"MAE\", \"RMSE\"])\n",
        "\n",
        "print(\"\\nüìä Full Results by Target:\")\n",
        "print(all_df)\n",
        "\n",
        "summary = all_df.groupby(\"Model\")[[\"R2\", \"MAE\", \"RMSE\"]].mean().reset_index()\n",
        "print(\"\\nüìà Average Performance Across Targets:\")\n",
        "print(summary.sort_values(\"R2\", ascending=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-dYQkd98Df2",
        "outputId": "770470fb-ee4c-4dcd-e0ea-9a436edf59bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üìä Full Results by Target:\n",
            "                Model             Target        R2        MAE      RMSE\n",
            "0   Linear Regression          TANKLEVEL  0.999995   0.001503  0.038764\n",
            "1       Random Forest          TANKLEVEL  0.999987   0.004911  0.070079\n",
            "2             XGBoost          TANKLEVEL  0.999976   0.008254  0.090853\n",
            "3        MLPRegressor          TANKLEVEL  0.999853   0.016569  0.128720\n",
            "4           Keras DNN          TANKLEVEL  0.999989   0.004387  0.066235\n",
            "5   Linear Regression         OUTPUTFLOW  0.982759  11.540820  3.397178\n",
            "6       Random Forest         OUTPUTFLOW  0.993423   1.702474  1.304789\n",
            "7             XGBoost         OUTPUTFLOW  0.994071   1.767303  1.329399\n",
            "8        MLPRegressor         OUTPUTFLOW  0.990095   7.525835  2.743326\n",
            "9           Keras DNN         OUTPUTFLOW  0.981772   4.897777  2.213092\n",
            "10  Linear Regression  RESERVETANKVOLUME  0.999999   0.061581  0.248155\n",
            "11      Random Forest  RESERVETANKVOLUME  0.999987   0.290753  0.539215\n",
            "12            XGBoost  RESERVETANKVOLUME  0.999931   0.624361  0.790165\n",
            "13       MLPRegressor  RESERVETANKVOLUME  0.999999   0.084707  0.291045\n",
            "14          Keras DNN  RESERVETANKVOLUME  0.999998   0.177453  0.421252\n",
            "\n",
            "üìà Average Performance Across Targets:\n",
            "               Model        R2       MAE      RMSE\n",
            "4            XGBoost  0.997993  0.799973  0.736806\n",
            "3      Random Forest  0.997799  0.666046  0.638028\n",
            "2       MLPRegressor  0.996649  2.542370  1.054363\n",
            "1  Linear Regression  0.994251  3.867968  1.228032\n",
            "0          Keras DNN  0.993920  1.693206  0.900193\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# === Load datasets ===\n",
        "df_normal = pd.read_excel(\"/content/drive/MyDrive/CWDS_DEMO/Areal Dataset/Data without attacks.xlsx\")\n",
        "df_attack = pd.read_excel(\"/content/drive/MyDrive/CWDS_DEMO/Areal Dataset/Data with attacks.xlsx\")\n",
        "\n",
        "print(\"‚úÖ Normal shape:\", df_normal.shape)\n",
        "print(\"‚úÖ Attack shape:\", df_attack.shape)\n",
        "\n",
        "# Merge datasets\n",
        "df_full = pd.concat([df_normal, df_attack], ignore_index=True)\n",
        "print(\"‚úÖ Merged dataset shape:\", df_full.shape)\n",
        "\n",
        "# === Prepare columns ===\n",
        "sensor_cols = [col for col in df_full.columns if col not in ['Date', 'Hours', 'ATT_FLAG','DATETIME']]\n",
        "\n",
        "# === Define target sensors ===\n",
        "target_sensors = ['TANKLEVEL', 'OUTPUTFLOW', 'RESERVETANKVOLUME']\n",
        "\n",
        "# === Evaluate helper ===\n",
        "def evaluate(model_name, y_true, y_pred):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    return model_name, r2, mae, rmse\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for target_sensor in target_sensors:\n",
        "    print(f\"\\n=== Target: {target_sensor}_next ===\")\n",
        "    df_temp = df_full.copy()\n",
        "\n",
        "    # Create prediction target (next time step)\n",
        "    df_temp[f'{target_sensor}_next'] = df_temp[target_sensor].shift(-1)\n",
        "    df_temp.dropna(inplace=True)\n",
        "\n",
        "    X = df_temp[sensor_cols]\n",
        "    y = df_temp[f'{target_sensor}_next']\n",
        "\n",
        "    # Scale features\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Split train/test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Linear Regression\n",
        "    lr = LinearRegression().fit(X_train, y_train)\n",
        "    results.append(evaluate(\"Linear Regression\", y_test, lr.predict(X_test)))\n",
        "\n",
        "    # Random Forest\n",
        "    rf = RandomForestRegressor(n_estimators=100, random_state=42).fit(X_train, y_train)\n",
        "    results.append(evaluate(\"Random Forest\", y_test, rf.predict(X_test)))\n",
        "\n",
        "    # XGBoost\n",
        "    xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42).fit(X_train, y_train)\n",
        "    results.append(evaluate(\"XGBoost\", y_test, xgb.predict(X_test)))\n",
        "\n",
        "    # MLP Regressor\n",
        "    mlp = MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42).fit(X_train, y_train)\n",
        "    results.append(evaluate(\"MLPRegressor\", y_test, mlp.predict(X_test)))\n",
        "\n",
        "    # Keras DNN\n",
        "    dnn = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    dnn.compile(optimizer='adam', loss='mae')\n",
        "    dnn.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "    y_pred_dnn = dnn.predict(X_test).flatten()\n",
        "    results.append(evaluate(\"Keras DNN\", y_test, y_pred_dnn))\n",
        "\n",
        "    # Save results\n",
        "    for res in results:\n",
        "        all_results.append((res[0], target_sensor, res[1], res[2], res[3]))\n",
        "\n",
        "# Create results dataframe\n",
        "results_df = pd.DataFrame(all_results, columns=[\"Model\", \"Target\", \"R2\", \"MAE\", \"RMSE\"])\n",
        "print(\"\\nüìä Results per target:\")\n",
        "print(results_df)\n",
        "\n",
        "# Average results\n",
        "avg_results = results_df.groupby(\"Model\")[[\"R2\", \"MAE\", \"RMSE\"]].mean().reset_index()\n",
        "print(\"\\nüìà Average performance across targets:\")\n",
        "print(avg_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "Oh9VsVtfxIEw",
        "outputId": "0a7fadd5-5c9c-4506-bfc5-f9f3d1958b8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Normal shape: (11521, 40)\n",
            "‚úÖ Attack shape: (11747, 41)\n",
            "‚úÖ Merged dataset shape: (23268, 42)\n",
            "\n",
            "=== Target: TANKLEVEL_next ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "DTypePromotionError",
          "evalue": "The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Float64DType'>)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mDTypePromotionError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-20c10bbb6828>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;31m# Scale features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mscaler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0mX_scaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;31m# Split train/test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0;31m# Reset internal state before fitting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 447\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0m_fit_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer_skip_nested_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         \u001b[0mfirst_pass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"n_samples_seen_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2944\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    929\u001b[0m         )\n\u001b[1;32m    930\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdtype_iter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 931\u001b[0;31m             \u001b[0mdtype_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    932\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpandas_requires_conversion\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdtypes_orig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    933\u001b[0m             \u001b[0;31m# Force object if any of the dtypes is an object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mDTypePromotionError\u001b[0m: The DType <class 'numpy.dtypes.DateTime64DType'> could not be promoted by <class 'numpy.dtypes.Float64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dtypes.Int64DType'>, <class 'numpy.dt..."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# === Load ATTACK dataset only ===\n",
        "df_attack = pd.read_excel(\"/content/drive/MyDrive/CWDS_DEMO/Areal Dataset/Data with attacks.xlsx\")\n",
        "print(\"‚úÖ Attack dataset shape:\", df_attack.shape)\n",
        "\n",
        "# === Drop non-numeric columns (datetime, labels) ===\n",
        "non_features = ['Date', 'Hours', 'ATT_FLAG']\n",
        "if 'DATETIME' in df_attack.columns:\n",
        "    non_features.append('DATETIME')\n",
        "\n",
        "sensor_cols = [col for col in df_attack.columns if col not in non_features]\n",
        "print(f\"‚úÖ Selected sensor features: {sensor_cols}\")\n",
        "\n",
        "# === Define target sensors ===\n",
        "target_sensors = ['TANKLEVEL', 'OUTPUTFLOW', 'RESERVETANKVOLUME']\n",
        "\n",
        "# === Evaluate helper ===\n",
        "def evaluate(model_name, y_true, y_pred):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    return model_name, r2, mae, rmse\n",
        "\n",
        "all_results = []\n",
        "\n",
        "for target_sensor in target_sensors:\n",
        "    print(f\"\\n=== Target: {target_sensor}_next ===\")\n",
        "    df_temp = df_attack.copy()\n",
        "\n",
        "    # Create next-step prediction target\n",
        "    df_temp[f\"{target_sensor}_next\"] = df_temp[target_sensor].shift(-1)\n",
        "    df_temp.dropna(inplace=True)\n",
        "\n",
        "    X = df_temp[sensor_cols]\n",
        "    y = df_temp[f\"{target_sensor}_next\"]\n",
        "\n",
        "    # Ensure X has only numeric columns\n",
        "    X_numeric = X.select_dtypes(include=[np.number])\n",
        "    print(f\"‚úÖ X shape for {target_sensor}: {X_numeric.shape}\")\n",
        "\n",
        "    # Scale features\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X_numeric)\n",
        "\n",
        "    # Train/test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Linear Regression\n",
        "    lr = LinearRegression().fit(X_train, y_train)\n",
        "    results.append(evaluate(\"Linear Regression\", y_test, lr.predict(X_test)))\n",
        "\n",
        "    # Random Forest\n",
        "    rf = RandomForestRegressor(n_estimators=100, random_state=42).fit(X_train, y_train)\n",
        "    results.append(evaluate(\"Random Forest\", y_test, rf.predict(X_test)))\n",
        "\n",
        "    # XGBoost\n",
        "    xgb = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42).fit(X_train, y_train)\n",
        "    results.append(evaluate(\"XGBoost\", y_test, xgb.predict(X_test)))\n",
        "\n",
        "    # SAVE the model + features\n",
        "    joblib.dump(xgb, f\"xgb_model_{target_sensor.lower()}.pkl\")\n",
        "    joblib.dump(sensor_cols, f\"xgb_features_{target_sensor.lower()}.pkl\")\n",
        "    print(f\"‚úÖ Saved model and features for {target_sensor}\")\n",
        "\n",
        "    # MLP Regressor\n",
        "    mlp = MLPRegressor(hidden_layer_sizes=(64, 32), max_iter=500, random_state=42).fit(X_train, y_train)\n",
        "    results.append(evaluate(\"MLPRegressor\", y_test, mlp.predict(X_test)))\n",
        "\n",
        "    # Keras DNN\n",
        "    dnn = Sequential([\n",
        "        Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "        Dense(32, activation='relu'),\n",
        "        Dense(1)\n",
        "    ])\n",
        "    dnn.compile(optimizer='adam', loss='mae')\n",
        "    dnn.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "    y_pred_dnn = dnn.predict(X_test).flatten()\n",
        "    results.append(evaluate(\"Keras DNN\", y_test, y_pred_dnn))\n",
        "\n",
        "    # Save results\n",
        "    for res in results:\n",
        "        all_results.append((res[0], target_sensor, res[1], res[2], res[3]))\n",
        "\n",
        "# Create results dataframe\n",
        "results_df = pd.DataFrame(all_results, columns=[\"Model\", \"Target\", \"R2\", \"MAE\", \"RMSE\"])\n",
        "print(\"\\nüìä Results per target:\")\n",
        "print(results_df)\n",
        "\n",
        "# Average results\n",
        "avg_results = results_df.groupby(\"Model\")[[\"R2\", \"MAE\", \"RMSE\"]].mean().reset_index()\n",
        "print(\"\\nüìà Average performance across targets:\")\n",
        "print(avg_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUmno3eonAZH",
        "outputId": "1ba22d65-9018-4362-deda-fb30c248d015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Attack dataset shape: (11747, 41)\n",
            "‚úÖ Selected sensor features: ['CONSUMERFLOW.DEFECT', 'CONSUMERFLOW', 'DEFECT.PUMP1', 'DEFECT.PUMP2', 'DEFECT.PUMP3', 'DEFECT.PUMP4', 'ENTRYFLOW.DEFECT', 'ENTRYFLOW', 'FLOW.PUMP1', 'FLOW.PUMP2', 'FLOW.PUMP3', 'FLOW.PUMP4', 'INPUTVALVE.CLOSE', 'INPUTVALVE.DEFECT.OPEN', 'INPUTVALVE.FDC.CLOSE', 'INPUTVALVE.FDC.OPEN', 'INPUTVALVE.OPEN', 'OUTPUTFLOW.DEFECT', 'OUTPUTFLOW', 'OUTPUTVALVE.CLOSE', 'OUTPUTVALVE.DEFECT.OPEN', 'OUTPUTVALVE.FDC.CLOSE', 'OUTPUTVALVE.FDC.OPEN', 'OUTPUTVALVE.OPEN', 'RESERVETANKVOLUME.DEFECT', 'RESERVETANKVOLUME', 'STATE.PUMP1', 'STATE.PUMP2', 'STATE.PUMP3', 'STATE.PUMP4', 'TANKLEVEL.DEFECT', 'TANKLEVEL.HIGH', 'TANKLEVEL.LOW', 'TANKLEVEL', 'CURRENT.FLOW.PUMP1', 'CURRENT.FLOW.PUMP2', 'CURRENT.FLOW.PUMP3', 'CURRENT.FLOW.PUMP4']\n",
            "\n",
            "=== Target: TANKLEVEL_next ===\n",
            "‚úÖ X shape for TANKLEVEL: (11746, 38)\n",
            "‚úÖ Saved model and features for TANKLEVEL\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m74/74\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "=== Target: OUTPUTFLOW_next ===\n",
            "‚úÖ X shape for OUTPUTFLOW: (11746, 38)\n",
            "‚úÖ Saved model and features for OUTPUTFLOW\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m74/74\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "=== Target: RESERVETANKVOLUME_next ===\n",
            "‚úÖ X shape for RESERVETANKVOLUME: (11746, 38)\n",
            "‚úÖ Saved model and features for RESERVETANKVOLUME\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m74/74\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
            "\n",
            "üìä Results per target:\n",
            "                Model             Target        R2        MAE       RMSE\n",
            "0   Linear Regression          TANKLEVEL  0.999929   0.003938   0.017489\n",
            "1       Random Forest          TANKLEVEL  0.999743   0.007598   0.033173\n",
            "2             XGBoost          TANKLEVEL  0.999761   0.010768   0.031979\n",
            "3        MLPRegressor          TANKLEVEL  0.999852   0.014393   0.025135\n",
            "4           Keras DNN          TANKLEVEL  0.999830   0.016835   0.026992\n",
            "5   Linear Regression         OUTPUTFLOW  0.983734  11.498421  45.425436\n",
            "6       Random Forest         OUTPUTFLOW  0.992886   1.724894  30.041569\n",
            "7             XGBoost         OUTPUTFLOW  0.994157   1.451572  27.224812\n",
            "8        MLPRegressor         OUTPUTFLOW  0.985660   8.988362  42.651763\n",
            "9           Keras DNN         OUTPUTFLOW  0.982567   5.240664  47.026741\n",
            "10  Linear Regression  RESERVETANKVOLUME  0.999969   0.205285   0.940493\n",
            "11      Random Forest  RESERVETANKVOLUME  0.998902   0.458163   5.575883\n",
            "12            XGBoost  RESERVETANKVOLUME  0.997605   0.924486   8.234696\n",
            "13       MLPRegressor  RESERVETANKVOLUME  0.999959   0.538388   1.072161\n",
            "14          Keras DNN  RESERVETANKVOLUME  0.999940   0.921407   1.305885\n",
            "\n",
            "üìà Average performance across targets:\n",
            "               Model        R2       MAE       RMSE\n",
            "0          Keras DNN  0.994112  2.059635  16.119873\n",
            "1  Linear Regression  0.994544  3.902548  15.461139\n",
            "2       MLPRegressor  0.995157  3.180381  14.583020\n",
            "3      Random Forest  0.997177  0.730218  11.883541\n",
            "4            XGBoost  0.997174  0.795609  11.830496\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import joblib\n",
        "\n",
        "joblib.dump(xgb, f\"xgb_model_{target_sensor.lower()}.pkl\")\n",
        "joblib.dump(sensor_cols, f\"xgb_features_{target_sensor.lower()}.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mm9qUBcpBHkf",
        "outputId": "1921a986-10e7-4adb-c5ea-c12ff71df320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['xgb_features_reservetankvolume.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest"
      ],
      "metadata": {
        "id": "w7eZvjMLdKpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# === Load ATTACK dataset only ===\n",
        "df_attack = pd.read_excel(\"/content/drive/MyDrive/CWDS_DEMO/Areal Dataset/Data with attacks.xlsx\")\n",
        "print(\"‚úÖ Attack dataset shape:\", df_attack.shape)\n",
        "\n",
        "# === Drop non-numeric columns (datetime, labels) ===\n",
        "non_features = ['Date', 'Hours', 'ATT_FLAG']\n",
        "if 'DATETIME' in df_attack.columns:\n",
        "    non_features.append('DATETIME')\n",
        "\n",
        "sensor_cols = [col for col in df_attack.columns if col not in non_features]\n",
        "print(f\"‚úÖ Selected sensor features: {sensor_cols}\")\n",
        "\n",
        "# === Define target sensors ===\n",
        "target_sensors = ['TANKLEVEL', 'OUTPUTFLOW', 'RESERVETANKVOLUME']\n",
        "\n",
        "# === Evaluate helper ===\n",
        "def evaluate(model_name, y_true, y_pred):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    return model_name, r2, mae, rmse"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIWcugszd-je",
        "outputId": "6f55c416-4697-4fce-e58e-163fa4493fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Attack dataset shape: (11747, 41)\n",
            "‚úÖ Selected sensor features: ['CONSUMERFLOW.DEFECT', 'CONSUMERFLOW', 'DEFECT.PUMP1', 'DEFECT.PUMP2', 'DEFECT.PUMP3', 'DEFECT.PUMP4', 'ENTRYFLOW.DEFECT', 'ENTRYFLOW', 'FLOW.PUMP1', 'FLOW.PUMP2', 'FLOW.PUMP3', 'FLOW.PUMP4', 'INPUTVALVE.CLOSE', 'INPUTVALVE.DEFECT.OPEN', 'INPUTVALVE.FDC.CLOSE', 'INPUTVALVE.FDC.OPEN', 'INPUTVALVE.OPEN', 'OUTPUTFLOW.DEFECT', 'OUTPUTFLOW', 'OUTPUTVALVE.CLOSE', 'OUTPUTVALVE.DEFECT.OPEN', 'OUTPUTVALVE.FDC.CLOSE', 'OUTPUTVALVE.FDC.OPEN', 'OUTPUTVALVE.OPEN', 'RESERVETANKVOLUME.DEFECT', 'RESERVETANKVOLUME', 'STATE.PUMP1', 'STATE.PUMP2', 'STATE.PUMP3', 'STATE.PUMP4', 'TANKLEVEL.DEFECT', 'TANKLEVEL.HIGH', 'TANKLEVEL.LOW', 'TANKLEVEL', 'CURRENT.FLOW.PUMP1', 'CURRENT.FLOW.PUMP2', 'CURRENT.FLOW.PUMP3', 'CURRENT.FLOW.PUMP4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "df_temp = df_attack.copy()\n",
        "for target_sensor in target_sensors:\n",
        "    df_temp[f\"{target_sensor}_next\"] = df_temp[target_sensor].shift(-1)\n",
        "df_temp.dropna(inplace=True)\n",
        "\n",
        "X = df_temp[sensor_cols]\n",
        "Y = df_temp[[f\"{target}_next\" for target in target_sensors]]\n",
        "\n",
        "scaler = MinMaxScaler().fit(X)\n",
        "X_scaled = scaler.transform(X)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "rf_multi = RandomForestRegressor(n_estimators=100, random_state=42).fit(X_train, Y_train)\n",
        "\n",
        "\n",
        "joblib.dump(rf_multi, \"rf_multi_model.pkl\")\n",
        "joblib.dump(sensor_cols, \"rf_multi_features.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmIOxESVdMiM",
        "outputId": "056b0d11-6284-4d69-a213-efee47cdcecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['rf_multi_features.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "retraining\n"
      ],
      "metadata": {
        "id": "KrIKG0yYKEOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from xgboost import XGBRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# === Load ATTACK dataset only ===\n",
        "df_attack = pd.read_excel(\"/content/drive/MyDrive/CWDS_DEMO/Areal Dataset/Data with attacks.xlsx\")\n",
        "print(\"‚úÖ Attack dataset shape:\", df_attack.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaYfvbHGKGUa",
        "outputId": "3cee2d26-945d-468f-81d2-8ec2cc217fcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Attack dataset shape: (11747, 41)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selected_features = [\n",
        "    'CONSUMERFLOW.DEFECT', 'CONSUMERFLOW', 'DEFECT.PUMP1', 'DEFECT.PUMP2', 'DEFECT.PUMP3', 'DEFECT.PUMP4',\n",
        "    'ENTRYFLOW.DEFECT', 'ENTRYFLOW', 'FLOW.PUMP1', 'FLOW.PUMP2', 'FLOW.PUMP3', 'FLOW.PUMP4',\n",
        "    'INPUTVALVE.CLOSE', 'INPUTVALVE.DEFECT.OPEN', 'INPUTVALVE.FDC.CLOSE', 'INPUTVALVE.FDC.OPEN', 'INPUTVALVE.OPEN',\n",
        "    'OUTPUTFLOW.DEFECT', 'OUTPUTFLOW', 'OUTPUTVALVE.CLOSE', 'OUTPUTVALVE.DEFECT.OPEN',\n",
        "    'OUTPUTVALVE.FDC.CLOSE', 'OUTPUTVALVE.FDC.OPEN', 'OUTPUTVALVE.OPEN', 'RESERVETANKVOLUME.DEFECT',\n",
        "    'RESERVETANKVOLUME', 'STATE.PUMP1', 'STATE.PUMP2', 'STATE.PUMP3', 'STATE.PUMP4', 'TANKLEVEL.DEFECT',\n",
        "    'TANKLEVEL.HIGH', 'TANKLEVEL.LOW', 'TANKLEVEL', 'CURRENT.FLOW.PUMP1', 'CURRENT.FLOW.PUMP2',\n",
        "    'CURRENT.FLOW.PUMP3', 'CURRENT.FLOW.PUMP4'\n",
        "]"
      ],
      "metadata": {
        "id": "MofHJAUBLQj9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import VarianceThreshold\n",
        "# Extract feature matrix\n",
        "X = df_attack[selected_features]\n",
        "\n",
        "# Apply variance threshold filtering\n",
        "threshold = 1e-3  # filter out near-constant features\n",
        "selector = VarianceThreshold(threshold=threshold)\n",
        "X_filtered = selector.fit_transform(X)\n",
        "\n",
        "# Get the names of retained features\n",
        "retained_feature_names = [feature for i, feature in enumerate(X.columns) if selector.get_support()[i]]\n",
        "\n",
        "# Optional: print or use the filtered DataFrame\n",
        "X_filtered_df = pd.DataFrame(X_filtered, columns=retained_feature_names)\n",
        "print(\"Retained Features:\", retained_feature_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjqRlT4UKUsr",
        "outputId": "626134a6-479e-486b-a36a-41e0e9e0cd6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retained Features: ['CONSUMERFLOW', 'ENTRYFLOW', 'FLOW.PUMP1', 'FLOW.PUMP2', 'FLOW.PUMP3', 'OUTPUTFLOW', 'OUTPUTVALVE.FDC.CLOSE', 'OUTPUTVALVE.FDC.OPEN', 'RESERVETANKVOLUME.DEFECT', 'RESERVETANKVOLUME', 'STATE.PUMP1', 'STATE.PUMP2', 'STATE.PUMP3', 'STATE.PUMP4', 'TANKLEVEL.DEFECT', 'TANKLEVEL.HIGH', 'TANKLEVEL', 'CURRENT.FLOW.PUMP1', 'CURRENT.FLOW.PUMP2', 'CURRENT.FLOW.PUMP3', 'CURRENT.FLOW.PUMP4']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "\n",
        "# Retained features from earlier step\n",
        "features = ['CONSUMERFLOW', 'ENTRYFLOW', 'FLOW.PUMP1', 'FLOW.PUMP2', 'FLOW.PUMP3', 'OUTPUTFLOW',\n",
        "            'OUTPUTVALVE.FDC.CLOSE', 'OUTPUTVALVE.FDC.OPEN', 'RESERVETANKVOLUME.DEFECT',\n",
        "            'RESERVETANKVOLUME', 'STATE.PUMP1', 'STATE.PUMP2', 'STATE.PUMP3', 'STATE.PUMP4',\n",
        "            'TANKLEVEL.DEFECT', 'TANKLEVEL.HIGH', 'TANKLEVEL',\n",
        "            'CURRENT.FLOW.PUMP1', 'CURRENT.FLOW.PUMP2', 'CURRENT.FLOW.PUMP3', 'CURRENT.FLOW.PUMP4']\n",
        "\n",
        "targets = ['TANKLEVEL', 'OUTPUTFLOW', 'RESERVETANKVOLUME']\n",
        "results = []\n",
        "\n",
        "for target in targets:\n",
        "    df_temp = df_attack.copy()\n",
        "    df_temp[f\"{target}_next\"] = df_temp[target].shift(-1)\n",
        "    df_temp.dropna(inplace=True)\n",
        "\n",
        "    X = df_temp[features]\n",
        "    y = df_temp[f\"{target}_next\"]\n",
        "\n",
        "    # Scale features\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Train XGBoost model\n",
        "    model = XGBRegressor(n_estimators=100, learning_rate=0.1, max_depth=5, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Save model using XGBoost's native format (version-safe)\n",
        "    model.save_model(f\"xgb_model_{target.lower()}.json\")\n",
        "\n",
        "    # Save feature list\n",
        "    joblib.dump(features, f\"xgb_features_{target.lower()}.pkl\")\n",
        "\n",
        "    # Evaluate\n",
        "    y_pred = model.predict(X_test)\n",
        "    r2 = r2_score(y_test, y_pred)\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "\n",
        "    results.append((target, r2, mae, rmse))\n",
        "\n",
        "# Show results\n",
        "print(\"\\n=== Model Performance ===\")\n",
        "for target, r2, mae, rmse in results:\n",
        "    print(f\"{target}: R2={r2:.4f}, MAE={mae:.4f}, RMSE={rmse:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OLorqgSXN9Qs",
        "outputId": "cc5c813b-afb0-4942-dd7e-49b70629074f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Model Performance ===\n",
            "TANKLEVEL: R2=0.9998, MAE=0.0108, RMSE=0.0320\n",
            "OUTPUTFLOW: R2=0.9942, MAE=1.4516, RMSE=27.2248\n",
            "RESERVETANKVOLUME: R2=0.9976, MAE=0.9245, RMSE=8.2347\n"
          ]
        }
      ]
    }
  ]
}